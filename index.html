<html>
  <head><title>International Workshop on Arm-based HPC: Practice and Experience (IWAHPCE-2025)</title>
   <link href="style.css" rel="stylesheet" />
  </head>
  <body>
    <table style="width:1000px;margin:0px">
      <tr><td style="width:300px;margin:0px;background-color:#000000;">
	  <center><img src="text899.png"></center>
	</td><td style="width:10px;margin:0px"></td>
	<td>
	  <div class="title">
	    International Workshop on Arm-based HPC: Practice and Experience (IWAHPCE-2025)</div><br>
	    &nbsp;<br>
	  to be held in conjunction with The International Conference on High Performance Computing in Asia-Pacific Region (<a href="https://event1.nchc.org.tw/hpcasia2025/">HPC Asia 2025</a>), Hsinchu, Taiwan, Feb.19-21, 2025.<br>
	  	    &nbsp;<br>
	  <!--------------------------------------------->
	  <div class="subtitle">Workshop Overview</div><br>
This workshop aims to provide the opportunity to share the practice and experience of high-performance computing systems using the Arm architecture and their performance and applications.The last few years have seen an explosion of 64-bit Arm-based processors targeted toward server and infrastructure workloads, often specializing in a specific domain such as HPC, cloud, and machine learning. Fujitsu‚Äôs A64FX and Marvell‚Äôs ThunderX2 have been used in several large-scale HPC systems, and Amazon‚Äôs Graviton2 has been adopted by Amazon EC2. Moreover, Amazon‚Äôs Graviton3, NVIDIA Grace CPU Superchip, and SiPearl‚Äôs Rhea system-on-chip are recently announced or become accessible.Sharing the practice and experiences using these Arm-based processors will contribute to advancing high-performance computing technology for newly designed systems using these new emerging Arm-based processors.<br><br>
	  <!--------------------------------------------->
	  <div class="subtitle">Program</div><br>
	  &nbsp;&nbsp;&nbsp;&nbsp;<table class="timetable">
	    		<tr><td valign="top" witdth="50px">&nbsp;&nbsp;&nbsp;</td><td valign="top" witdth="100px"></td><td valign="top" witdth="200px"></td></tr>
			<tr><td valign="top">09:00-09:10&nbsp;&nbsp;&nbsp;</td><td valign="top">Miwako Tsuji</td><td valign="top"><b>Opening Remarks</b></td></tr>
		<tr><td valign="top">09:10-10:00&nbsp;&nbsp;&nbsp;</td><td valign="top">Koichi Shirahata</td><td valign="top"><b>Invited Talk: Fugaku-LLM: A Large Language Model Trained on the Supercomputer Fugaku</b> (<a href="https://arm-hpc-user-group.github.io/iwahpce-2025/20250219_HPC_Asia_2025_Shirahata_distribution.pdf">slides</a>)<br>
Abstract: 
While not initially designed for large-scale deep learning models like Large Language Models (LLMs), Japan's flagship supercomputer, Fugaku, provided a unique opportunity. This work details the optimization of deep learning frameworks for distributed parallel execution on Fugaku, creating a high-performance computing environment for LLM training. A novel LLM was trained from scratch using a large dataset primarily focused on Japanese text.
</td></tr>
			<tr><td valign="top">10:00-10:30&nbsp;&nbsp;&nbsp;</td><td valign="top">Shinji  Sumimoto, Takashi  Arakawa, Yoshio  Sakaguchi, Hiroya  Matsuba, Satoshi  Ohshima, Hisashi  Yashiro, Toshihiro  Hanawa, Kengo  Nakajima</td><td valign="top"><b>Accelerating Heterogeneous Coupling Computing with WaitIO Using RDMA</b> <br>Abstract: In this paper, we propose communication libraries WaitIO-Verbs
andWaitIO-Tofu using RDMA communication to speed up communication
performance in the h3-OpenSYS/WaitIO (WaitIO) library,
which can connect multiple MPI programs across multiple heterogeneous
systems. It is important to use industry-standard communication
methods for communication between heterogeneous systems,
and WaitIO implements WaitIO-Socket and WaitIO-File, which use
POSIX-based specifications for socket and file IO. However, since
POSIX specifications generally use system calls, there is a possibility
that sufficient performance may not be obtained depending on
the system. Therefore, to further speed up communication, we implemented
WaitIO-Verbs and WaitIO-Tofu using user-level RDMA
using industry-standard or default system communication specifications.
As a result of implementation and evaluation, we achieved
high communication performance and application performance.
WaitIO achieved high application performance even between multiple
heterogeneous clusters, which MPI could not achieve.</td></tr>
			<tr><td valign="top">10:30-11:00&nbsp;&nbsp;&nbsp;</td><td valign="top"></td><td valign="top">Break</td></tr>
			<tr><td valign="top">11:00-11:30&nbsp;&nbsp;&nbsp;</td><td valign="top">Oscar  Hernandez, Thomas  Wang, Wael  Elwasif, Filippo  Spiga, Francesca  Tartaglione, Markus  Eisenbach, Ross  Miller</td><td valign="top"><b>Preliminary Study on Fine-Grained Power and Energy Measurements on Grace Hopper GH200 with Open-Source Performance Tools</b> <br>Abstract: The increasing adoption of tightly integrated, heterogeneous architectures,
combined with the slowdown of Moore‚Äôs law, has made
application power and energy-driven optimizations critical to efficiently
use high-performance computing systems. This paper introduces
a newly developed open-source toolkit that seamlessly
integrates the Linux real-time hardware monitoring program hwmon
with the Performance Application Programming Interface and the
Score-P performance measurement system, thereby enabling finegrained
power and energy measurements for high-performance
computing applications. Our primary target platform is theWombat
test bed, which is a system based on the NVIDIA GH200 superchip.
The toolkit can capture transient power peaks with high temporal
resolution (50 ms) and, thanks to Score-P integration, can map
power metrics to specific code regions, thereby providing actionable
information on power-intensive operations and inefficiencies.
The toolkit also provides a holistic view of both the power and the
energy consumption of the entire GH200 superchip by covering all
major components: the Grace CPU, the Hopper GPU, and the I/O
subsystem. Experiments that use Locally Self-consistent Multiple
Scattering, which is an application for first-principles calculations
of materials developed at Oak Ridge National Laboratory, have
demonstrated the tool‚Äôs ability to identify transient power spikes
and uncover opportunities for energy-aware optimizations. Additionally,
we introduce a Python-based utility for converting Open
Trace Format 2 traces to Parquet format, thus enabling advanced
data analysis for numerical integration methods applied to power
data for accurate energy profiling.</td></tr>
			<tr><td valign="top">11:30-12:00&nbsp;&nbsp;&nbsp;</td><td valign="top">David  Carlson, Nikolay  Simakov, Rodrigo  Ristow Hadlich, Anthony  Curtis, Joshua  Martin, Gaurav  Verma, Smeet  Chheda, Firat  Coskun, Raul  Gonzalez, Daniel  Wood, Feng  Zhang, Robert  Harrison, Eva  Siegmann</td><td valign="top"><b>The AmpereOne A192-32X in Perspective: Benchmarking a New Standard</b> <br>Abstract:  This study presents a comprehensive benchmarking analysis of the Arm-based AmpereOne A192-32X CPU, a high-performance but
low power processor designed for cloud-native workloads characterized by high core occupancy, imperfectly-vectorized or even
pure scalar software, limited need for high floating-point performance, and, increasingly, AI inference. These traits also characterize
much of academic research computing . Hence a thorough investigation of this novel CPU seeking to characterize its strengths and
weaknesses on academic workloads, including traditional HPC codes for which it was not designed, will shed light on its relevance in a
research setting. We report comparative analyses with contemporary CPUs (Intel Sapphire Rapids, AMD EPYC, NVIDIA Grace-Grace)
and illustrate AmpereOne‚Äôs architectural advantages in handling parallel workloads and optimizing power consumption. The CPUs
are compared in terms of performance and power consumption using a wide range of applications covering different workloads and
disciplines.</td></tr>
			<tr><td valign="top">12:00-12:30&nbsp;&nbsp;&nbsp;</td><td valign="top">Xuanzhengbo  Ren, Yuta  Kawai, Hirofumi  Tomita, Seiya  Nishizawa, Takahiro  Katagiri, Tetsuya  Hoshino, Daichi  Mukunoki, Masatoshi  Kawai, Toru  Nagai</td><td valign="top"><b>Performance Evaluation of Loop Body Splitting for Fast Modal Filtering in SCALE-DG on A64FX</b> <br>Abstract: Modern general-purpose central processing units (CPUs) benefit
from the integration of Single Instruction, Multiple Data (SIMD) architectures.
Scalable Vector Extensions (SVE) is one of Arm‚Äôs SIMD
architectures designed for HPC. Fujitsu‚Äôs A64FX is the first Armbased
processor to incorporate hardware-implemented SVE alongside
High-Bandwidth Memory 2 (HBM2). However, in the A64FX,
the high latencies of SIMD instructions, such as fused multiply-add
(FMA), combined with the limited capacity of reservation stations,
can result in inefficient Out-of-Order (OoO) execution. SCALEDG
is an atmospheric dynamical core that uses the discontinuous
Galerkin Method (DGM). Modal filtering, an essential procedure
in SCALE-DG, has an optimized version called fast modal filtering,
which suffers from the OoO execution issue due to dot products
with long vector lengths in the loop body. This issue can be alleviated
by splitting the loop body into multiple parts. However, since
the vector length of the dot product varies with the polynomial
order (ùëÉ) in SCALE-DG, it remains unclear how the number of
splits affects the performance of fast modal filtering. In this paper,
we present an evaluation across ùëÉ values in the range of 3 to 11
with different splitting numbers and combinations. The results indicate
that when ùëÉ ‚â§ 7, performance degraded in most cases, with
only a few cases achieving positive speedups (1.01x to 1.02x) after
splitting the loop body. For 8 ‚â§ ùëÉ ‚â§ 11, splitting had a consistently
positive impact. The 3-way splitting was identified as the optimal
configuration, achieving speedups of 1.15x to 1.26x.</td></tr>
		</table>
<br><br>
	  <!--------------------------------------------->		
	  <div class="subtitle">Topics</div>
	  <br>
In particular, this workshop will focus on the following topics of interest:<br>
&nbsp;&nbsp;&nbsp;-&nbsp;HPC Applications<br> 
&nbsp;&nbsp;&nbsp;-&nbsp;Performance Analysis, Performance Modeling & Measurement<br>
&nbsp;&nbsp;&nbsp;-&nbsp;SVE Vectorization analysis<br>
&nbsp;&nbsp;&nbsp;-&nbsp;Programming Models & System Softoware<br>
&nbsp;&nbsp;&nbsp;-&nbsp;Networking and accelerators such as GPUs<br>
&nbsp;&nbsp;&nbsp;-&nbsp;Artificial Intelligence and Machine Learning<br>
&nbsp;&nbsp;&nbsp;-&nbsp;Emerging Technologies<br>
	  <br>
	  <!--------------------------------------------->
	  <div class="subtitle">Paper Submissions</div>
	  <br>
All papers must be original and not simultaneously submitted to another journal or conference. The following paper categories are welcome:<br>
&nbsp;&nbsp;&nbsp;-&nbsp;Full papers Manuscripts must be at most 18 pages in one-column submission PDF format including figures and references<br>
&nbsp;&nbsp;&nbsp;-&nbsp;Short papers Manuscripts must be at most 10 pages in one-column submission PDF format including figures and references<br>
The paper format is described in the Paper Submission section of <a href="https://event1.nchc.org.tw/hpcasia2025/papers.html">HPCAsia2025</a><br>
Please note that the paper format for the submission (one-column) is different from the one of the camera ready (2-column). Fore more detail, please refer https://www.acm.org/publications/authors/submissions<br>
All submissions will be peer-reviewed by the PC members. Papers will be accepted to presentations in a workshop. The review process is double-blind. Please do NOT include the name of authors, etc... <br>
			  <br>
	  <!--------------------------------------------->
	  <div class="subtitle">Important Dates</div>
	  <br>
&nbsp;&nbsp;&nbsp;-&nbsp;Full Paper Submission (via <s>Linklings</s>Easychair): 23th Dec 2024<br>
&nbsp;&nbsp;&nbsp;-&nbsp;Notification: 12th Jan 2025<br>
&nbsp;&nbsp;&nbsp;-&nbsp;Camera ready: 15th Jan 2025<br>
			  <br>
	  <!--------------------------------------------->

          <div class="subtitle">Submission Site</div>
	  <br>
			<u><a href="https://easychair.org/conferences/?conf=iwahpce2025">https://easychair.org/conferences/?conf=iwahpce2025</a></u> <br>
			  <br>
	  <!--------------------------------------------->
	  <!--------------------------------------------->

          <div class="subtitle">Registration and Open Access Fee</div>
	  <br>
&nbsp;&nbsp;&nbsp;-&nbsp;At least one author must register for the conference<br>
	  &nbsp;&nbsp;&nbsp;-&nbsp;Based on the ACM's new Open Access publication policy, the <b>publication fee</b> will be contacted with and collected by ACM directly from the authors.<br>
	  &nbsp;&nbsp;&nbsp;-&nbsp;For more details about the ACM Open Access policy and fee, please refer <a href="https://authors.acm.org/open-access">here</a> and <a href="https://www.acm.org/publications/icps/author-guidance">here</a>. 
	  <br>
&nbsp;&nbsp;&nbsp;-&nbsp;However, if your institution is a member of the ACM Open program, the paper will be published with no charge , as indicated on the web site. The list of participating institutions of ACM Open can be viewed https://libraries.acm.org/acmopen/open-participants.   
	  <br>
			  <br>
	  <!--------------------------------------------->

          <div class="subtitle">Organizers and Program Committee</div>
	  <br>
		<b>Organizer and Workshop Chair</b><br>
    Miwako Tsuji, RIKEN R-CCS<br>
    Eva Siegmann, Stony Brook University<br>
    Filippo Spiga, NVIDIA<br>
<br>
<b>Program Committee</b><br>
		Conrad Hillairet &nbsp;&nbsp;Arm<br>
Csaba Csoma &nbsp;&nbsp;ASW<br>
Estela Suarez &nbsp;&nbsp;JSC<br>
Eva Siegmann&nbsp;&nbsp;Stony Brook University<br>
Fabio Banchelli &nbsp;&nbsp;BSC<br>
Filippo Spiga&nbsp;&nbsp;NVIDIA<br>
Gilles Fourestey &nbsp;&nbsp;EPFL<br>
Jens Domke &nbsp;&nbsp;RIKEN R-CCS<br>
John Cazes &nbsp;&nbsp;TACC<br>
Luca Fedeli &nbsp;&nbsp;CEA<br>
Min Li &nbsp;&nbsp;Huawei<br>
Miwako Tsuji&nbsp;&nbsp; RIKEN R-CCS<br>
Tetsuya Odajima &nbsp;&nbsp;Fujitsu<br>
Wael Elwas&nbsp;&nbsp;ORNL<br>
Yuetsu Kodama&nbsp;&nbsp; RIKEN R-CCS<br>
	</td>
  </tr>
  </table>
  </body>
  
</html>
